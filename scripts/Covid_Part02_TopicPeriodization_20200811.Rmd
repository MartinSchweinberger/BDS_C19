---
title: "A real-time corpus-based analysis of the discourse around COVID19 in the Australian Twittersphere - Part 2: Topic Modelling"
author: "Anonymous"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: word_document
bibliography: bibliography.bib
link-citations: yes
---

This document details the data analysis for the project *A real-time corpus-based analysis of the discourse around COVID19 in the Australian Twittersphere*. 

This part of the analysis aims to find topics in the COVID discourse and to identify periods in the COVID discourae based on the distribution of topics.

WARNING: this part of the analysis requires manual processing!

The analysis is initiated by clearing the workspace, setting options, and activating packages as well as functions.

```{r covtwit_02_01, echo=T, eval = T, message=FALSE, warning=FALSE}
# clean current workspace
rm(list=ls(all=T))  
# load libraries
library(tokenizers)
library(tm)
library(stringi)
library(tidytext)
library(tidyverse)
library(SnowballC)
library(here)
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=10000)
# define image directory
imageDirectory<-"images"
```

Load twitter data

```{r covtwit_02_02, echo=T, eval = T, message=FALSE, warning=FALSE}
covidtwitterdata <- read.delim(here::here("data", "cleantweets_classified_periods.txt"), comment.char = "", quote = "") %>%
  dplyr::filter(SVM_class == "Covid") %>%
  dplyr::rename(doc_id = Id, text = Text)
# inspect data
head(covidtwitterdata); nrow(covidtwitterdata)
```


Create corpus and DTM

```{r covtwit_02_03, echo=T, eval = T, message=FALSE, warning=FALSE}
# create corpus
corpus <- Corpus(DataframeSource(covidtwitterdata))
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 10
DTM <- DocumentTermMatrix(corpus, 
                          control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
```

Due to vocabulary pruning, DTM may have empty rows (problematic!): remove empty docs from DTM and metadata

```{r covtwit_02_04, echo=T, eval = T, message=FALSE, warning=FALSE}
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
covidtwitterdata <- covidtwitterdata[sel_idx, ]
```

# Determine optimal number of topics

The determinination of the optimal number of topics follows @murzintcev2020idealtopics.

```{r covtwit_02_05, echo=T, eval = T, message=FALSE, warning=FALSE}
# load packages
library(topicmodels)
library(ldatuning)
# create sample data set
ldatuneset <- covidtwitterdata
ldatuneCorpus <- corpus <- Corpus(DataframeSource(ldatuneset))
ldatuneDTM <- DocumentTermMatrix(ldatuneCorpus,
                                 control = list(bounds = list(global = c(minimumFrequency, Inf))))
sel_idx <- slam::row_sums(ldatuneDTM) > 0
ldatuneDTM <- ldatuneDTM[sel_idx, ]
# create models with different number of topics
result <- FindTopicsNumber(
  ldatuneDTM,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
```

Plot results (best number of topics: lowest CaoJuan2009, highest Griffith2004)

```{r covtwit_02_06, echo=T, eval = T, message=FALSE, warning=FALSE}
FindTopicsNumber_plot(result)
```

Results are volatile but there is a dip in minimizers and a peak with maximizers at 9. Thus, we select K = 10.

```{r covtwit_02_07, echo=T, eval = T, message=FALSE, warning=FALSE}
# number of topics
K <- 5
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
```

Extract LDA results

```{r covtwit_02_08, echo=T, eval = T, message=FALSE, warning=FALSE}
# have a look a some of the results (posterior distributions)
LDA_Result <- posterior(topicModel)
# topics are probability distribtions over the entire vocabulary
beta <- LDA_Result$terms   # get beta from results
# for every document we have a probaility distribution of its contained topics
theta <- as.data.frame(LDA_Result$topics)
# extract dominant topic for each tweet
covidtwitterdata$DominantTopic <- colnames(theta)[apply(theta,1,which.max)]
head(covidtwitterdata)
```

Write function for extracting distinctive terms per topic

```{r covtwit_02_09, echo=T, eval = T, message=FALSE, warning=FALSE}
ExtractAllTopicKeywords <- function(covidtwitterdata, Topic = 1){
  source(here::here("scripts", "CoocStatzFisher.R"))
  TopicTerms <- covidtwitterdata %>%
  unnest_tokens(Word, text, token = "tweets") %>%
  dplyr::mutate(DominantTopic = ifelse(DominantTopic == Topic, "Target", "NonTarget")) %>%
  group_by(DominantTopic) %>% 
  count(Word, sort = TRUE) %>% 
  spread(DominantTopic, n) %>% 
  replace(is.na(.), 0) %>%
  mutate(Total = Target+NonTarget) %>%
  dplyr::filter(Total > 10) %>%
  dplyr::mutate(TotalTarget = sum(Target),
                TotalNonTarget = sum(NonTarget),
                NRows = length(Word)) %>%
  dplyr::select(-Total)
  # extract keywords
  keywords <- CoocStatzFisher(TopicTerms)
  sigkeywords <- keywords %>%
    dplyr::filter(CorrSignificance != "n.s.",
                  Type == "Overuse") %>%
    dplyr::arrange(-phi)
  return(sigkeywords)
}
```

Extract keyterms for topics

```{r covtwit_02_10, echo=T, eval = T, message=FALSE, warning=FALSE}
# apply function
AllSigKeywordsTopic01 <- ExtractAllTopicKeywords(covidtwitterdata, 1)
AllSigKeywordsTopic02 <- ExtractAllTopicKeywords(covidtwitterdata, 2)
AllSigKeywordsTopic03 <- ExtractAllTopicKeywords(covidtwitterdata, 3)
AllSigKeywordsTopic04 <- ExtractAllTopicKeywords(covidtwitterdata, 4)
AllSigKeywordsTopic05 <- ExtractAllTopicKeywords(covidtwitterdata, 5)
# determine number of key terms
NTerms <- 10
# combine tables
AllSigKeywordsTopic <- rbind(AllSigKeywordsTopic01[1:NTerms,], 
                             AllSigKeywordsTopic02[1:NTerms,], 
                             AllSigKeywordsTopic03[1:NTerms,], 
                             AllSigKeywordsTopic04[1:NTerms,], 
                             AllSigKeywordsTopic05[1:NTerms,])
# add Topic column
AllSigKeywordsTopic$Topic <- c(rep("Topic1", NTerms),
                               rep("Topic2", NTerms),
                               rep("Topic3", NTerms),
                               rep("Topic4", NTerms),
                               rep("Topic5", NTerms)) 
# clean table
AllSigKeywordsTopic <- AllSigKeywordsTopic %>%
  dplyr::select(-NonTarget, -TotalTarget, -TotalNonTarget, -RateTarget, -RateNonTarget, 
                -Type, -x2, -p, -Target)
# inspect data
head(AllSigKeywordsTopic)
```

```{r covtwit_02_11, echo=T, eval = T, message=FALSE, warning=FALSE}
KeywordsTopic <- AllSigKeywordsTopic %>%
  dplyr::mutate(Word = paste0(Word, " (", phi, "***)")) %>%
  dplyr::select(-CorrSignificance, -phi)
# save data to disc
write.table(KeywordsTopic, 
            here::here("tables", "KeywordsTopic.txt"), 
            sep = "\t", row.names = F, quote = F)
# inspect table
head(KeywordsTopic)
```

Write function for extracting the top six distinctive terms per topic

```{r covtwit_02_12, echo=T, eval = T, message=FALSE, warning=FALSE}
ExtractTopicKeywords <- function(covidtwitterdata, Topic = 1){
  source(here::here("scripts", "CoocStatzFisher.R"))
  TopicTerms <- covidtwitterdata %>%
  unnest_tokens(Word, text, token = "tweets") %>%
  dplyr::mutate(DominantTopic = ifelse(DominantTopic == Topic, "Target", "NonTarget")) %>%
  group_by(DominantTopic) %>% 
  count(Word, sort = TRUE) %>% 
  spread(DominantTopic, n) %>% 
  replace(is.na(.), 0) %>%
  mutate(Total = Target+NonTarget) %>%
  dplyr::filter(Total > 10) %>%
  dplyr::mutate(TotalTarget = sum(Target),
                TotalNonTarget = sum(NonTarget),
                NRows = length(Word)) %>%
  dplyr::select(-Total)
  # extract keywords
  keywords <- CoocStatzFisher(TopicTerms)
  sigkeywords <- keywords %>%
    dplyr::filter(CorrSignificance != "n.s.",
                  Type == "Overuse") %>%
    dplyr::arrange(-phi)
  return(paste(sigkeywords$Word[1:6], collapse = "|"))
}
```

Extract keyterms for topics

```{r covtwit_02_13, echo=T, eval = T, message=FALSE, warning=FALSE}
# apply function
SigKeywordsTopic01 <- ExtractTopicKeywords(covidtwitterdata, 1)
SigKeywordsTopic02 <- ExtractTopicKeywords(covidtwitterdata, 2)
SigKeywordsTopic03 <- ExtractTopicKeywords(covidtwitterdata, 3)
SigKeywordsTopic04 <- ExtractTopicKeywords(covidtwitterdata, 4)
SigKeywordsTopic05 <- ExtractTopicKeywords(covidtwitterdata, 5)
# inspect data
head(SigKeywordsTopic01)
```

Create topic names based on the most distinctive terms for each topic

```{r covtwit_02_14, echo=T, eval = T, message=FALSE, warning=FALSE}
topictermsls <- c(SigKeywordsTopic01, SigKeywordsTopic02, SigKeywordsTopic03,
                  SigKeywordsTopic04, SigKeywordsTopic05)
#topicNames <- paste("Topic", str_pad(1:5, 2, pad = "0"), topictermsls, sep = "_")
topicNames <- paste("Topic", 1:5, topictermsls, sep = "_")
topicNames <- str_replace_all(topicNames, "c_", "c")
topicNames
```

Manually created topic names

```{r covtwit_02_15, echo=T, eval = T, message=FALSE, warning=FALSE}
topicNames_manual <- c("Topic1_MEDICAL", "Topic2_INTERNATIONAL", "Topic3_RESTRICTIONS|HOME",
                       "Topic4_SPREAD", "Topic5_ECONOMY")
```



Add topics names to data

```{r covtwit_02_16, echo=T, eval = T, message=FALSE, warning=FALSE}
covidtwitterdata <- covidtwitterdata %>%
  dplyr::mutate(Topic = ifelse(DominantTopic == "1", topicNames_manual[1],
                        ifelse(DominantTopic == "2", topicNames_manual[2],
                        ifelse(DominantTopic == "3", topicNames_manual[3],
                        ifelse(DominantTopic == "4", topicNames_manual[4],
                               topicNames_manual[5]))))) %>%
  dplyr::rename(Period = Period_Keyword,
                Text = text,
                Id = doc_id)
# save data
write.table(covidtwitterdata, here::here("data", "covidtwitter2020data_topics_periods.txt"), 
            sep = "\t", row.names = F, quote = F)
# inspect data
head(covidtwitterdata)
```

Create Probability of Topics per Phase table

```{r covtwit_02_17, echo=T, eval = T, message=FALSE, warning=FALSE}
topicspd <- covidtwitterdata %>%
  dplyr::select(Period, Topic) %>%
  dplyr::group_by(Period) %>%
  dplyr::mutate(NoTweets = n()) %>%
  dplyr::group_by(Period, Topic) %>%
  dplyr::summarise(NoTweets = unique(NoTweets),
                   FrequencyTopic = n(),
                   Probability = FrequencyTopic/NoTweets*100) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(Period = str_remove_all(Period, ".*_"),
                Period = as.numeric(Period))
# inspect data
head(topicspd)

```

Visualize results

```{r covtwit_02_18, echo=T, eval = T, message=FALSE, warning=FALSE}
require(pals)
ggplot(topicspd, aes(x=Period, y=Probability, fill=Topic)) +
  guides(fill=guide_legend(nrow = 3)) +
  geom_bar(stat = "identity") + 
  labs(y = "Percent (of COVID-19 tweets)", x = "Phase") + 
  scale_x_continuous(breaks = seq(1, 7, 1), labels= seq(1, 7, 1)) +
  scale_fill_manual(labels = names(table(topicspd$Topic)),
                      breaks = names(table(topicspd$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  theme_set(theme_bw(base_size = 12)) +
  theme(legend.position="top",
        axis.text.x = element_text(angle = 0, hjust = 1, size = 10)) +
  ggsave(file = here::here("images", "TopicModel.png"),
         height = 6,  width = 5, dpi = 320)
```

```{r covtwit_02_19, echo=T, eval = T, message=FALSE, warning=FALSE}
topicspd2 <- topicspd %>%
  dplyr::mutate(Topic = factor(Topic)) %>%
  dplyr::group_by(Period, Topic) %>%
  dplyr::summarise(Percent = mean(Probability))

ggplot(topicspd2, aes(x=Period, y=Percent, color = Topic, linetype = Topic)) + 
  geom_smooth(span = .75, se = F) + 
  guides(color=guide_legend(nrow = 3)) +
  theme_set(theme_bw(base_size = 12)) +
  scale_colour_manual(labels = names(table(topicspd2$Topic)),
                      breaks = names(table(topicspd2$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  scale_linetype_manual(labels = names(table(topicspd2$Topic)),
                        breaks = names(table(topicspd2$Topic)),
                        values = seq(1, 5, 1), 
                        name = "") +
  labs(y = "Percent (of COVID-19 tweets)", x = "Phase") + 
  scale_x_continuous(breaks = seq(1, 7, 1), 
                     labels= seq(1, 7, 1), 
                     limits = c(1, 7)) +
  scale_y_continuous(limits = c(0, 50)) +
  theme(legend.position="top", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 0, hjust = 1, size = 10)) +
  ggsave(file = here::here("images", "TopicModel_smooth.png"),
         height = 6,  width = 5, dpi = 320)
```

```{r covtwit_02_20, echo=T, eval = T, message=FALSE, warning=FALSE}
ggplot(topicspd2, aes(x=Period, y=Percent, color = Topic, linetype = Topic)) + 
  geom_line(size = 1.25) + 
  guides(color=guide_legend(nrow = 5)) +
  scale_colour_manual(labels = names(table(topicspd2$Topic)),
                      breaks = names(table(topicspd2$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  scale_linetype_manual(labels = names(table(topicspd2$Topic)),
                        breaks = names(table(topicspd2$Topic)),
                        values = seq(1, 5, 1), 
                        name = "") +
  labs(y = "Percent (of COVID-19 tweets)", x = "Phase") + 
  scale_x_continuous(breaks = seq(1, 7, 1), 
                     labels= seq(1, 7, 1), 
                     limits = c(1, 7)) +
  scale_y_continuous(limits = c(0, 50)) +
  theme(legend.position="top", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 0, hjust = 1)) +
  ggsave(file = here::here("images", "TopicModel_line.png"),
         height = 6,  width = 5, dpi = 320)
```

# Outro

```{r}
sessionInfo()
```

